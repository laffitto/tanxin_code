{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) approach to Matric Factorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import heapq\n",
    "\n",
    "from time import time\n",
    "from scipy.sparse import load_npz\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import get_train_instances, get_scores\n",
    "from gmf import train, evaluate, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \".\"\n",
    "dataname = \"neuralcf_split.npz\"\n",
    "train_matrix = \"neuralcf_train_sparse.npz\"\n",
    "modeldir = \"models\"\n",
    "layers = [128, 64, 32, 16] # first layer is n_emb*2\n",
    "dropouts = [0., 0., 0.0] # len(dropouts) = len(layers)-1\n",
    "l2reg = 0.\n",
    "n_emb = int(layers[0]/2)\n",
    "batch_size = 512\n",
    "epochs = 1\n",
    "learner = \"Adam\"\n",
    "lr = 0.01\n",
    "lr_scheduler = None\n",
    "validate_every = 1 # boolean\n",
    "topk = 10\n",
    "n_neg = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Again, rather simple, user and item embeddings, concatenated and pass through a series of dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_user, n_item, layers, dropouts):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)\n",
    "        self.dropouts = dropouts\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "\n",
    "        self.embeddings_user = nn.Embedding(n_user, int(layers[0]/2))\n",
    "        self.embeddings_item = nn.Embedding(n_item, int(layers[0]/2))\n",
    "\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(1,self.n_layers):\n",
    "            self.mlp.add_module(\"linear%d\" %i, nn.Linear(layers[i-1],layers[i]))\n",
    "            self.mlp.add_module(\"relu%d\" %i, torch.nn.ReLU())\n",
    "            self.mlp.add_module(\"dropout%d\" %i , torch.nn.Dropout(p=dropouts[i-1]))\n",
    "\n",
    "        self.out = nn.Linear(in_features=layers[-1], out_features=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "\n",
    "        user_emb = self.embeddings_user(users)\n",
    "        item_emb = self.embeddings_item(items)\n",
    "        emb_vector = torch.cat([user_emb,item_emb], dim=1)\n",
    "        emb_vector = self.mlp(emb_vector)\n",
    "        preds = torch.sigmoid(self.out(emb_vector))\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 64, 32, 16]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = MLP(10, 10, layers, dropouts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluating is exactly the same as in `GMF` so we move directly to Run the Model\n",
    "\n",
    "### Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(os.path.join(datadir, dataname))\n",
    "train_ratings = load_npz(os.path.join(datadir, train_matrix)).todok()\n",
    "test_ratings, negatives = dataset['test_negative'], dataset['negatives']\n",
    "n_users, n_items = dataset['n_users'].item(), dataset['n_items'].item()\n",
    "\n",
    "test_loader = DataLoader(dataset=test_ratings,\n",
    "    batch_size=1000,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "model = MLP(n_users, n_items, layers, dropouts)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2reg)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aaa1d7f468a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     loss = train(model, criterion, optimizer, scheduler, epoch, batch_size,\n\u001b[0;32m----> 6\u001b[0;31m         use_cuda, train_ratings, negatives, n_items, n_neg)\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/course.txkj/advanced.ml.published/P5.NeuralCollaborativeFiltering/gmf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, scheduler, epoch, batch_size, use_cuda, train_ratings, negatives, n_items, n_neg)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_hr, best_ndcgm, best_iter=0,0,0\n",
    "scheduler = None\n",
    "for epoch in range(1,epochs+1):\n",
    "    t1 = time()\n",
    "    loss = train(model, criterion, optimizer, scheduler, epoch, batch_size,\n",
    "        use_cuda, train_ratings, negatives, n_items, n_neg)\n",
    "    t2 = time()\n",
    "    if epoch % validate_every == 0:\n",
    "        (hr, ndcg) = evaluate(model, test_loader, use_cuda, topk)\n",
    "        print(\"Epoch: {} {:.2f}s, LOSS = {:.4f}, HR = {:.4f}, NDCG = {:.4f}, validated in {:.2f}s\".\n",
    "            format(epoch, t2-t1, loss, hr, ndcg, time()-t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

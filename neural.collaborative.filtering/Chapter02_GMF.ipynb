{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generalized Matrix Factorization (GMF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import heapq\n",
    "\n",
    "from time import time\n",
    "from scipy.sparse import load_npz\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \".\"\n",
    "dataname = \"neuralcf_split.npz\"\n",
    "train_matrix = \"neuralcf_train_sparse.npz\"\n",
    "modeldir = \"models\"\n",
    "n_emb = 8\n",
    "batch_size = 512\n",
    "epochs = 1\n",
    "learner = \"Adam\"\n",
    "lr = 0.03\n",
    "validate_every = 1\n",
    "topk = 10\n",
    "n_neg = 4 # number of negative examples during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "定义模型, 对用户和物品的One-Hot编码的输入进行Embedding, 然后对应元素相乘. 将相乘得到的结果向量通过一个全连接层, 通过sigmoid激活函数得到输出."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, n_emb=8):\n",
    "        super(GMF, self).__init__()\n",
    "\n",
    "        self.n_emb = n_emb\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "\n",
    "        self.embeddings_user = nn.Embedding(n_user, n_emb)\n",
    "        self.embeddings_item = nn.Embedding(n_item, n_emb)\n",
    "        self.out = nn.Linear(in_features=n_emb, out_features=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "\n",
    "        user_emb = self.embeddings_user(users)\n",
    "        item_emb = self.embeddings_item(items)\n",
    "        prod = user_emb*item_emb\n",
    "        preds = torch.sigmoid(self.out(prod))\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = GMF(n_user=10, n_item=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "按照论文的方法, 我们训练的时候忽略实际的rating, 目标为 1/0 (取决于用户是否观看了此电影)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, negatives, n_items, n_neg):\n",
    "    \"\"\"\n",
    "    Select n_neg never seen movies per movie seen\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    train: scipy.sparse.dok.dok_matrix\n",
    "        sparse key-based matrix \n",
    "    negatives: np.ndarray\n",
    "        array of (n_user, 99) movies the user never rated that are used for testing\n",
    "    n_neg: int\n",
    "        number of negative (i.e. non-rated) movies per positive (i.e. rated)\n",
    "    n_items: int\n",
    "        number of items in the dataset\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    train_w_negative: np.ndarray\n",
    "        array with (1+n_neg) entries per user\n",
    "    \"\"\"\n",
    "    user, item, labels = [],[],[]\n",
    "    for (u, i), r in train.items():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # negative examples used for testing\n",
    "        for _ in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in negatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    train_w_negative = np.vstack([user,item,labels]).T\n",
    "    assert train_w_negative.shape[0] == (len(train) + len(train)*n_neg)\n",
    "    return train_w_negative.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于训练的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, epoch, batch_size,\n",
    "          train_ratings, negatives, n_items, n_neg):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Build a training dataset with n_neg instances per positive instance\n",
    "    train_dataset = get_train_instances(train_ratings,\n",
    "        negatives,\n",
    "        n_items,\n",
    "        n_neg)\n",
    "\n",
    "    # Build the corresponding loader\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=True)\n",
    "\n",
    "    # From here in advance is a pretty standard training phase if you are familiar with pytorch\n",
    "    train_steps = (len(train_loader.dataset) // train_loader.batch_size) + 1\n",
    "    running_loss=0\n",
    "    for data in train_loader:\n",
    "        users = data[:,0]\n",
    "        items = data[:,1]\n",
    "        labels = data[:,2].float()\n",
    "        if use_cuda:\n",
    "            users, items, labels = users.cuda(), items.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        preds =  model(users, items)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        loss = criterion(preds.squeeze(1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss/train_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "#### 评价指标1: $Hit Rate (HR)$\n",
    "\n",
    "$\\mathrm{HR}=\\frac{\\# \\text { hits }}{\\# \\text { users }}$\n",
    "\n",
    "$HR$是目前$TOP-N$推荐研究中十分流行的评价指标，其公式如上所示，其中$#users$是用户总数，而$#hits$是测试集中的item出现在$Top-N$推荐列表中的用户数量。\n",
    "\n",
    "#### 评价指标2: $NDCG(Normalized Discounted Cumulative Gain)$\n",
    "\n",
    "N：归一化，D：衰减率，C：累加，G：熵（关键）：归一化的，带有衰减函数的，再带有累加的熵。\n",
    "\n",
    "说到$NDCG$就需要从$CG$开始说起。\n",
    "\n",
    "$CG$（cumulative gain，累计增益）可以用于评价基于打分/评分的个性推荐系统。假设我们推荐k个物品，这个推荐列表的$CG_k$计算公式如下：\n",
    "\n",
    "$CG_{k}=\\sum_{i=1}^{k} \\mathrm{rel}_{i}$\n",
    "\n",
    "$\\mathrm{rel}_{i}$表示第$k$个物品的相关性或者评分。假设我们共推荐$k$个电影，$\\mathrm{rel}_{i}$可以是用户对第$i$部电影的评分。\n",
    "\n",
    "比如豆瓣给用户推荐了五部电影: M1,M2,M3,M4,M5. 该用户对这五部电影的评分分别是: 5, 3, 2, 1, 2\n",
    "\n",
    "那么这个推荐列表的CG等于$CG_5=5+3+2+1+2=13$.\n",
    "\n",
    "CG没有考虑推荐的次序，在此基础之后我们引入对物品顺序的考虑，就有了DCG(discounted CG)，折扣累积增益。公式如下：\n",
    "\n",
    "$DCG_{k}=\\sum_{i=1}^{k} \\frac{2^{\\mathrm{rel}_{i}}-1}{\\log_{2}(i+1)}$\n",
    "\n",
    "比如豆瓣给用户推荐了五部电影: M1,M2,M3,M4,M5. 该用户对这五部电影的评分分别是: 5, 3, 2, 1, 2. 那么这个推荐列表的DCG等于:\n",
    "\n",
    "$D C G_{5}=\\frac{2^{5}-1}{\\log _{2} 2}+\\frac{2^{3}-1}{\\log _{2} 3}+\\frac{2^{2}-1}{\\log _{2} 4}+\\frac{2^{1}-1}{\\log _{2} 5}+\\frac{2^{2}-1}{\\log _{2} 6}=31+4.4+1.5+0.4+1.2=38.5$\n",
    "\n",
    "DCG没有考虑到推荐列表和每个检索中真正有效结果个数，所以最后我们引入NDCG(normalized discounted CG)，顾名思义就是标准化之后的DCG。\n",
    "\n",
    "$NDCG_k=\\frac{DCG_k}{IDCG_k}$\n",
    "\n",
    "其中$IDCG$是指$ideal DCG$，也就是完美结果下的$DCG$。\n",
    "\n",
    "继续上面的例子，如果相关电影一共有5部: M1,M2,M3,M4,M5. 该用户对这七部电影的评分分别是: 5, 3, 2, 2, 4. 把这5部电影按评分排序的结果为: 5, 4, 3, 2, 2.\n",
    "\n",
    "这个情况下的完美$DCG$是:\n",
    "\n",
    "$I D C G_{5}=\\frac{2^{5}-1}{\\log _{2} 2}+\\frac{2^{4}-1}{\\log _{2} 3}+\\frac{2^{3}-1}{\\log _{2} 4}+\\frac{2^{2}-1}{\\log _{2} 5}+\\frac{2^{2}-1}{\\log _{2} 6}=31+9.5+3.5+1.3+1.2=46.5$\n",
    "\n",
    "如果某种推荐算法排序的方式为: 2, 5, 3, 4, 2, 那么其 $DCG_5$的值为:\n",
    "\n",
    "$D C G_{5}=\\frac{2^{2}-1}{\\log _{2} 2}+\\frac{2^{5}-1}{\\log _{2} 3}+\\frac{2^{3}-1}{\\log _{2} 4}+\\frac{2^{4}-1}{\\log _{2} 5}+\\frac{2^{2}-1}{\\log _{2} 6}=33.7$\n",
    "\n",
    "$NDCG_5=\\frac{DCG_5}{IDCG_5}=\\frac{33.7}{46.5}=0.72$\n",
    "\n",
    "$NDCG$是0到1的数，越接近1说明推荐越准确。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hitratio(ranklist, gtitem):\n",
    "    if gtitem in ranklist: return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_ndcg(ranklist, gtitem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtitem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_scores(items, preds, topk):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    items: List\n",
    "        list of 100 item ids where the 1st one is the rated one\n",
    "    preds: List\n",
    "        list of 100 predictions for those item ratings\n",
    "    topk: int\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    hr, ndcg: hit ratio and normalised discounted cumulative gain\n",
    "    \"\"\"\n",
    "    gtitem = items[0]\n",
    "\n",
    "    # the following 3 lines of code ensure that the fact that the 1st item is\n",
    "    # gtitem does not affect the final rank\n",
    "    randidx = np.arange(100)\n",
    "    np.random.shuffle(randidx)\n",
    "    items, preds = items[randidx], preds[randidx]\n",
    "\n",
    "    map_item_score = dict( zip(items, preds) )\n",
    "    ranklist = heapq.nlargest(topk, map_item_score, key=map_item_score.get)\n",
    "    hr = get_hitratio(ranklist, gtitem)\n",
    "    ndcg = get_ndcg(ranklist, gtitem)\n",
    "    return hr, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看一看随机排序的测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43, 87, 98, 60, 58, 94, 38, 68, 62, 66, 51, 52, 78, 91, 89, 37, 88,\n",
       "       65, 82, 13, 79, 54, 85, 47, 34, 29, 49, 28, 10, 57, 24, 63, 46, 12,\n",
       "       95, 36, 76, 39, 69,  4, 83, 74, 48, 71, 30,  0, 55, 26, 96, 22,  5,\n",
       "        6, 32, 92, 50, 93, 72, 56, 14, 73,  2, 19, 45, 15, 25, 16, 53,  3,\n",
       "       61,  8, 40, 90, 21, 11, 23, 18, 99, 81, 80, 44,  1, 41, 67, 27, 97,\n",
       "       77, 59, 86,  9, 33, 42, 84, 31, 35, 70, 64, 75, 20,  7, 17])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranklist = np.arange(100)\n",
    "np.random.shuffle(ranklist)\n",
    "ranklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 `gtitem` 返回的结果不排在前shi, 那么 HR@10 和 NDCG@10 的值为 0. HR@10 不会考虑前十中的具体位置, 只要目标在前十, 第一或者第十的位置得分一样. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(get_hitratio(ranklist[:10], ranklist[9]), get_hitratio(ranklist[:10], ranklist[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " NDCG@10 却要考虑具体的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect ranking, NDCG@10: 1.0\n",
      "Ranked 3rd position out of 10, NDCG@10: 0.5\n",
      "Ranked 10th position out of 10, NDCG@10: 0.289\n"
     ]
    }
   ],
   "source": [
    "print('Perfect ranking, NDCG@10: {}'.format(get_ndcg(ranklist[:10], ranklist[0])))\n",
    "print('Ranked 3rd position out of 10, NDCG@10: {}'.format(round(get_ndcg(ranklist[:10], ranklist[2]),3)))\n",
    "print('Ranked 10th position out of 10, NDCG@10: {}'.format(round(get_ndcg(ranklist[:10], ranklist[9]),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个评价指标会告诉我们 1) 我们的推荐算法能够将正确的目标排在前十  2) 排在前十中的位置是否足够靠前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = np.arange(100)\n",
    "np.random.shuffle(items)\n",
    "preds = np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [14, 71, 59, 31, 10, 56, 33, 17, 51, 30]\n"
     ]
    }
   ],
   "source": [
    "gtitem = items[0] # first item always the ranked item\n",
    "map_item_score = dict( zip(items, preds) )\n",
    "ranklist = heapq.nlargest(topk, map_item_score, key=map_item_score.get)\n",
    "print(gtitem, ranklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评价指标的代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, use_cuda, topk):\n",
    "    model.eval()\n",
    "    scores=[]\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            users = data[:,0]\n",
    "            items = data[:,1]\n",
    "            labels = data[:,2].float()\n",
    "            if use_cuda:\n",
    "                users, items, labels = users.cuda(), items.cuda(), labels.cuda()\n",
    "            preds = model(users, items)\n",
    "            items_cpu = items.cpu().numpy()\n",
    "            preds_cpu = preds.squeeze(1).detach().cpu().numpy()\n",
    "            litems=np.split(items_cpu, test_loader.batch_size//100)\n",
    "            lpreds=np.split(preds_cpu, test_loader.batch_size//100)\n",
    "            scores += [get_scores(it,pr,topk) for it,pr in zip(litems,lpreds)]\n",
    "    hits = [s[0] for s in scores]\n",
    "    ndcgs = [s[1] for s in scores]\n",
    "    return (np.array(hits).mean(),np.array(ndcgs).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(os.path.join(datadir, dataname))\n",
    "train_ratings = load_npz(os.path.join(datadir, train_matrix)).todok()\n",
    "test_ratings, negatives = dataset['test_negative'], dataset['negatives']\n",
    "n_users, n_items = dataset['n_users'].item(), dataset['n_items'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<123960x50051 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1573573 stored elements in Dictionary Of Keys format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0    522      5]\n",
      " [     0  47956      0]\n",
      " [     0  14615      0]\n",
      " ...\n",
      " [123959  33315      0]\n",
      " [123959   4177      0]\n",
      " [123959  35687      0]] (12396000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_ratings, test_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29459 19109 34089 ... 17966  1480 16748]\n",
      " [34588 22241  2349 ... 27721 43745 45694]\n",
      " [27128  8654 21294 ...  9084 19600 49654]\n",
      " ...\n",
      " [24574 33588 41445 ...  1237 38094 27275]\n",
      " [27996 32987  4678 ...  8274 48478 15518]\n",
      " [10353 23547 17883 ... 10273 45692 35687]] (123960, 99)\n"
     ]
    }
   ],
   "source": [
    "print(negatives, negatives.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123960, 50052)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_ratings,\n",
    "    # for speed purposes we use large test batch sizes. These will be broken in chunks of 100 during evaluation\n",
    "    batch_size=1000,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GMF(n_users, n_items, n_emb=n_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = ((len(train_ratings)+len(train_ratings)*n_neg)//batch_size)+1\n",
    "step_size = training_steps*3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 292.39s, LOSS = 0.5048, HR = 0.1084, NDCG = 0.0500, validated in 91.51s\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "best_hr, best_ndcgm, best_iter=0,0,0\n",
    "for epoch in range(1,epochs+1):\n",
    "    t1 = time()\n",
    "    loss = train(model, criterion, optimizer, scheduler, epoch, batch_size, \n",
    "                 train_ratings, negatives, n_items, n_neg)\n",
    "    t2 = time()\n",
    "    if epoch % validate_every == 0:\n",
    "        (hr, ndcg) = evaluate(model, test_loader, use_cuda, topk)\n",
    "        print(\"Epoch: {} {:.2f}s, LOSS = {:.4f}, HR = {:.4f}, NDCG = {:.4f}, validated in {:.2f}s\".\n",
    "            format(epoch, t2-t1, loss, hr, ndcg, time()-t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-pytorch]",
   "language": "python",
   "name": "conda-env-anaconda3-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
